{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è JESA Tender Evaluation System - Google Colab Version\n",
        "\n",
        "**AI-Powered Supplier Proposal Analysis using Local LLM (Llama 3.2)**\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Overview\n",
        "\n",
        "This notebook uses a local Large Language Model (Llama 3.2 3B) running on Google Colab to analyze supplier proposals and rank them based on multiple criteria.\n",
        "\n",
        "**Features:**\n",
        "- ‚úÖ Zero cost (uses Google Colab free tier)\n",
        "- ‚úÖ Complete data privacy (no external APIs)\n",
        "- ‚úÖ PDF document processing\n",
        "- ‚úÖ AI-powered analysis with local LLM\n",
        "- ‚úÖ Weighted scoring and ranking\n",
        "- ‚úÖ Excel export functionality\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "1. Technical Compliance (30%)\n",
        "2. Price Competitiveness (25%)\n",
        "3. Company Experience (20%)\n",
        "4. Timeline Feasibility (15%)\n",
        "5. Risk Assessment (10%)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Setup Instructions\n",
        "\n",
        "**IMPORTANT: Enable GPU First!**\n",
        "1. Click **Runtime** ‚Üí **Change runtime type**\n",
        "2. Select **Hardware accelerator**: **T4 GPU**\n",
        "3. Click **Save**\n",
        "\n",
        "**Then run all cells in order (or click Runtime ‚Üí Run all)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Cell 1: Install Required Packages\n",
        "\n",
        "This cell installs all necessary Python packages. **Run this first!**\n",
        "\n",
        "‚è±Ô∏è Expected time: ~2-3 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "print(\"This may take 2-3 minutes...\\\\n\")\n",
        "\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q pdfplumber PyPDF2\n",
        "!pip install -q pandas openpyxl\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "print(\"\\\\n‚úÖ All packages installed successfully!\")\n",
        "print(\"You can now proceed to the next cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Cell 2: Import Libraries\n",
        "\n",
        "Import all necessary Python libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import pdfplumber\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import io\n",
        "import os\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Cell 3: Load Llama 3.2 Model\n",
        "\n",
        "Load the Llama 3.2 3B model from Hugging Face.\n",
        "\n",
        "‚è±Ô∏è First time: ~2-4 minutes (downloads ~6GB)\n",
        "\n",
        "‚è±Ô∏è Subsequent runs: ~30 seconds (cached)\n",
        "\n",
        "**Note**: This uses the free Llama 3.2 3B model optimized for Colab's free tier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ü§ñ Loading Llama 3.2 3B model...\")\n",
        "print(\"This may take 2-4 minutes on first run (model will be cached for future use)\\\\n\")\n",
        "\n",
        "# Model selection\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Alternative: Use Phi-3 if Llama is not available\n",
        "# MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "try:\n",
        "    # Load tokenizer\n",
        "    print(\"üìù Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    \n",
        "    # Load model with optimizations for Colab\n",
        "    print(\"üß† Loading model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,  # Use half precision to save memory\n",
        "        device_map=\"auto\",           # Automatically use GPU\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"  # Use eager attention to avoid flash-attention warnings\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\n‚úÖ Model loaded successfully!\")\n",
        "    print(f\"üìä Model: {MODEL_NAME}\")\n",
        "    print(f\"üíæ Model size: ~6GB\")\n",
        "    print(f\"üéØ Device: {next(model.parameters()).device}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    print(\"\\\\nTrying alternative model (Phi-3)...\")\n",
        "    \n",
        "    MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"  # Use eager attention to avoid flash-attention warnings\n",
        "    )\n",
        "    \n",
        "    # Set pad token if not set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    print(\"\\\\n‚úÖ Alternative model (Phi-3) loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Cell 4: Define PDF Processing Functions\n",
        "\n",
        "Functions to extract text from PDF documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_file_bytes, filename=\"document.pdf\"):\n",
        "    \"\"\"Extract text from PDF file bytes.\"\"\"\n",
        "    try:\n",
        "        # Save bytes to temporary file\n",
        "        temp_path = f\"/tmp/{filename}\"\n",
        "        with open(temp_path, 'wb') as f:\n",
        "            f.write(pdf_file_bytes)\n",
        "        \n",
        "        # Extract text using pdfplumber\n",
        "        text_parts = []\n",
        "        with pdfplumber.open(temp_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_parts.append(page_text)\n",
        "        \n",
        "        # Clean up temp file\n",
        "        os.remove(temp_path)\n",
        "        \n",
        "        return '\\\\n\\\\n'.join(text_parts)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error extracting PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_file(file_bytes, filename):\n",
        "    \"\"\"Extract text from PDF or TXT file.\"\"\"\n",
        "    if filename.lower().endswith('.pdf'):\n",
        "        return extract_text_from_pdf(file_bytes, filename)\n",
        "    elif filename.lower().endswith('.txt'):\n",
        "        return file_bytes.decode('utf-8')\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Unsupported file type: {filename}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"‚úÖ PDF processing functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Cell 5: Define AI Analysis Functions\n",
        "\n",
        "Functions to analyze proposals using the local LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_proposal_with_llm(proposal_text, tender_requirements, supplier_name):\n",
        "    \"\"\"Analyze a supplier proposal using the local LLM.\"\"\"\n",
        "    \n",
        "    # Create a shorter, more focused prompt for faster processing\n",
        "    prompt = f\"\"\"Analyze this proposal and return JSON scores (0-100):\n",
        "\n",
        "Tender: {tender_requirements[:1000]}\n",
        "\n",
        "Proposal: {proposal_text[:1500]}\n",
        "\n",
        "Return JSON:\n",
        "{{\n",
        "  \"supplier_name\": \"{supplier_name}\",\n",
        "  \"criteria_scores\": {{\n",
        "    \"technical_compliance\": {{\"score\": 85, \"justification\": \"Brief explanation\", \"evidence\": []}},\n",
        "    \"price_competitiveness\": {{\"score\": 75, \"justification\": \"Brief explanation\", \"evidence\": []}},\n",
        "    \"company_experience\": {{\"score\": 90, \"justification\": \"Brief explanation\", \"evidence\": []}},\n",
        "    \"timeline_feasibility\": {{\"score\": 80, \"justification\": \"Brief explanation\", \"evidence\": []}},\n",
        "    \"risk_assessment\": {{\"score\": 85, \"justification\": \"Brief explanation\", \"evidence\": []}}\n",
        "  }},\n",
        "  \"final_score\": 83.0,\n",
        "  \"overall_summary\": \"Summary\",\n",
        "  \"red_flags\": [],\n",
        "  \"recommendations\": \"Recommendation\"\n",
        "}}\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # Generate response using the model with optimized settings\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "        \n",
        "        print(f\"   üß† Starting generation...\")\n",
        "        print(f\"   üìä Input length: {inputs['input_ids'].shape[1]} tokens\")\n",
        "        \n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=500,  # Reduced for faster generation\n",
        "                temperature=0.7,\n",
        "                do_sample=False,     # Greedy decoding for faster results\n",
        "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=False      # Disable cache to avoid DynamicCache error\n",
        "            )\n",
        "        \n",
        "        generation_time = time.time() - start_time\n",
        "        print(f\"   ‚è±Ô∏è Generation completed in {generation_time:.1f} seconds\")\n",
        "        \n",
        "        # Decode the response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"   üìù Response length: {len(response)} characters\")\n",
        "        print(f\"   üîç Response preview: {response[:100]}...\")\n",
        "        \n",
        "        # Extract JSON from response\n",
        "        json_start = response.find('{')\n",
        "        json_end = response.rfind('}') + 1\n",
        "        \n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_content = response[json_start:json_end]\n",
        "            \n",
        "            try:\n",
        "                result = json.loads(json_content)\n",
        "                result['status'] = 'success'\n",
        "                result['model_used'] = MODEL_NAME\n",
        "                result['analysis_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                return result\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"‚ö†Ô∏è JSON parsing failed for {supplier_name}\")\n",
        "                return create_fallback_result(supplier_name)\n",
        "        else:\n",
        "            return create_fallback_result(supplier_name)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing {supplier_name}: {e}\")\n",
        "        return create_fallback_result(supplier_name)\n",
        "\n",
        "def create_fallback_result(supplier_name):\n",
        "    \"\"\"Create a fallback result when analysis fails.\"\"\"\n",
        "    return {\n",
        "        'supplier_name': supplier_name,\n",
        "        'criteria_scores': {\n",
        "            'technical_compliance': {'score': 50, 'justification': 'Analysis incomplete', 'evidence': []},\n",
        "            'price_competitiveness': {'score': 50, 'justification': 'Analysis incomplete', 'evidence': []},\n",
        "            'company_experience': {'score': 50, 'justification': 'Analysis incomplete', 'evidence': []},\n",
        "            'timeline_feasibility': {'score': 50, 'justification': 'Analysis incomplete', 'evidence': []},\n",
        "            'risk_assessment': {'score': 50, 'justification': 'Analysis incomplete', 'evidence': []}\n",
        "        },\n",
        "        'final_score': 50.0,\n",
        "        'overall_summary': 'Analysis could not be completed',\n",
        "        'red_flags': ['Manual review required'],\n",
        "        'recommendations': 'Please review manually',\n",
        "        'status': 'error',\n",
        "        'model_used': MODEL_NAME,\n",
        "        'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Analysis functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Cell 6: Define Scoring and Ranking Functions\n",
        "\n",
        "Functions to calculate weighted scores and rank suppliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_weighted_score(analysis, weights):\n",
        "    \"\"\"Calculate weighted final score for a supplier.\"\"\"\n",
        "    criteria_scores = analysis.get('criteria_scores', {})\n",
        "    weighted_sum = 0.0\n",
        "    \n",
        "    for criterion, weight in weights.items():\n",
        "        criterion_data = criteria_scores.get(criterion, {})\n",
        "        if isinstance(criterion_data, dict):\n",
        "            score = criterion_data.get('score', 0)\n",
        "            weighted_sum += score * (weight / 100.0)\n",
        "    \n",
        "    return round(weighted_sum, 2)\n",
        "\n",
        "def rank_suppliers(analyses, weights):\n",
        "    \"\"\"Rank suppliers based on weighted scores.\"\"\"\n",
        "    ranked_suppliers = []\n",
        "    \n",
        "    for analysis in analyses:\n",
        "        weighted_score = calculate_weighted_score(analysis, weights)\n",
        "        supplier_data = analysis.copy()\n",
        "        supplier_data['weighted_score'] = weighted_score\n",
        "        ranked_suppliers.append(supplier_data)\n",
        "    \n",
        "    # Sort by weighted score (descending)\n",
        "    ranked_suppliers.sort(key=lambda x: x['weighted_score'], reverse=True)\n",
        "    \n",
        "    # Assign ranks\n",
        "    for i, supplier in enumerate(ranked_suppliers):\n",
        "        supplier['rank'] = i + 1\n",
        "    \n",
        "    return ranked_suppliers\n",
        "\n",
        "def export_to_excel(ranked_suppliers, weights, filename='tender_evaluation_results.xlsx'):\n",
        "    \"\"\"Export results to Excel file.\"\"\"\n",
        "    # Create rankings sheet\n",
        "    rankings_data = []\n",
        "    for supplier in ranked_suppliers:\n",
        "        criteria_scores = supplier.get('criteria_scores', {})\n",
        "        rankings_data.append({\n",
        "            'Rank': supplier.get('rank', 0),\n",
        "            'Supplier Name': supplier.get('supplier_name', 'Unknown'),\n",
        "            'Final Score': supplier.get('weighted_score', 0),\n",
        "            'Technical': criteria_scores.get('technical_compliance', {}).get('score', 0),\n",
        "            'Price': criteria_scores.get('price_competitiveness', {}).get('score', 0),\n",
        "            'Experience': criteria_scores.get('company_experience', {}).get('score', 0),\n",
        "            'Timeline': criteria_scores.get('timeline_feasibility', {}).get('score', 0),\n",
        "            'Risk': criteria_scores.get('risk_assessment', {}).get('score', 0)\n",
        "        })\n",
        "    \n",
        "    df_rankings = pd.DataFrame(rankings_data)\n",
        "    \n",
        "    # Save to Excel\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        df_rankings.to_excel(writer, sheet_name='Rankings', index=False)\n",
        "    \n",
        "    return filename\n",
        "\n",
        "print(\"‚úÖ Scoring and ranking functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Cell 7: Upload Tender Requirements Document\n",
        "\n",
        "Click the button below to upload your tender requirements document (PDF or TXT).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Upload Tender Requirements Document\")\n",
        "print(\"Please select your tender requirements file (PDF or TXT)\\\\n\")\n",
        "\n",
        "uploaded_tender = files.upload()\n",
        "\n",
        "# Process the uploaded file\n",
        "tender_requirements = \"\"\n",
        "tender_filename = \"\"\n",
        "\n",
        "for filename, file_bytes in uploaded_tender.items():\n",
        "    tender_filename = filename\n",
        "    print(f\"\\\\nüìÑ Processing: {filename}\")\n",
        "    tender_requirements = extract_text_from_file(file_bytes, filename)\n",
        "    print(f\"‚úÖ Extracted {len(tender_requirements)} characters\")\n",
        "    print(f\"\\\\nPreview (first 300 characters):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(tender_requirements[:300] + \"...\" if len(tender_requirements) > 300 else tender_requirements)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "if tender_requirements:\n",
        "    print(\"\\\\nüéâ Tender requirements loaded successfully!\")\n",
        "else:\n",
        "    print(\"\\\\n‚ùå Failed to load tender requirements. Please try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÑ Cell 8: Upload Supplier Proposals\n",
        "\n",
        "Click the button below to upload multiple supplier proposal documents (PDF or TXT).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìÑ Upload Supplier Proposals\")\n",
        "print(\"You can select multiple files at once\\\\n\")\n",
        "\n",
        "uploaded_proposals = files.upload()\n",
        "\n",
        "# Process all uploaded files\n",
        "supplier_proposals = []\n",
        "\n",
        "for filename, file_bytes in uploaded_proposals.items():\n",
        "    print(f\"\\\\nüìÑ Processing: {filename}\")\n",
        "    proposal_text = extract_text_from_file(file_bytes, filename)\n",
        "    \n",
        "    if proposal_text:\n",
        "        supplier_proposals.append({\n",
        "            'name': filename,\n",
        "            'text': proposal_text\n",
        "        })\n",
        "        print(f\"‚úÖ Extracted {len(proposal_text)} characters\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to extract text from {filename}\")\n",
        "\n",
        "print(f\"\\\\nüéâ Loaded {len(supplier_proposals)} supplier proposal(s)!\")\n",
        "for i, proposal in enumerate(supplier_proposals, 1):\n",
        "    print(f\"  {i}. {proposal['name']}: {len(proposal['text'])} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è Cell 9: Configure Evaluation Weights\n",
        "\n",
        "Set the importance of each evaluation criterion. **Weights must total 100%!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure evaluation weights (must total 100%)\n",
        "evaluation_weights = {\n",
        "    'technical_compliance': 30,\n",
        "    'price_competitiveness': 25,\n",
        "    'company_experience': 20,\n",
        "    'timeline_feasibility': 15,\n",
        "    'risk_assessment': 10\n",
        "}\n",
        "\n",
        "# Validate weights\n",
        "total_weight = sum(evaluation_weights.values())\n",
        "\n",
        "print(\"‚öñÔ∏è Evaluation Weights Configuration\")\n",
        "print(\"=\" * 40)\n",
        "for criterion, weight in evaluation_weights.items():\n",
        "    print(f\"  {criterion.replace('_', ' ').title()}: {weight}%\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total: {total_weight}%\")\n",
        "\n",
        "if abs(total_weight - 100.0) < 0.01:\n",
        "    print(\"‚úÖ Weights are valid (total 100%)\")\n",
        "else:\n",
        "    print(f\"‚ùå Weights must total 100%, currently {total_weight}%\")\n",
        "    print(\"Please adjust the weights above and run this cell again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Cell 10: Run AI Analysis\n",
        "\n",
        "Analyze all supplier proposals using the local LLM.\n",
        "\n",
        "‚è±Ô∏è Expected time: ~2-5 minutes per proposal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ü§ñ Starting AI Analysis...\")\n",
        "print(f\"Analyzing {len(supplier_proposals)} proposal(s)...\\\\n\")\n",
        "\n",
        "analysis_results = []\n",
        "\n",
        "for i, proposal in enumerate(supplier_proposals, 1):\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\"üìä Analyzing Proposal {i}/{len(supplier_proposals)}: {proposal['name']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = analyze_proposal_with_llm(\n",
        "        proposal_text=proposal['text'],\n",
        "        tender_requirements=tender_requirements,\n",
        "        supplier_name=proposal['name']\n",
        "    )\n",
        "    \n",
        "    analysis_results.append(result)\n",
        "    \n",
        "    if result.get('status') == 'success':\n",
        "        print(f\"‚úÖ Analysis completed for {proposal['name']}\")\n",
        "        print(f\"   Final Score: {result.get('final_score', 'N/A')}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Analysis incomplete for {proposal['name']}\")\n",
        "    \n",
        "    # Clear GPU cache to prevent memory issues\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\\\n\\\\nüéâ Analysis completed for all {len(analysis_results)} proposals!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÜ Cell 11: Rank Suppliers and Calculate Scores\n",
        "\n",
        "Calculate weighted scores and rank all suppliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üèÜ Ranking Suppliers...\\\\n\")\n",
        "\n",
        "# Rank suppliers\n",
        "ranked_suppliers = rank_suppliers(analysis_results, evaluation_weights)\n",
        "\n",
        "# Calculate summary statistics\n",
        "scores = [s.get('weighted_score', 0) for s in ranked_suppliers]\n",
        "\n",
        "print(\"üìä EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total Suppliers Evaluated: {len(ranked_suppliers)}\")\n",
        "print(f\"Average Score: {sum(scores) / len(scores):.1f}\")\n",
        "print(f\"Highest Score: {max(scores):.1f}\")\n",
        "print(f\"Lowest Score: {min(scores):.1f}\")\n",
        "print(f\"Score Range: {max(scores) - min(scores):.1f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\\\n‚úÖ Ranking completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Cell 12: Display Results - Rankings Table\n",
        "\n",
        "View the ranked suppliers with their scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üèÜ SUPPLIER RANKINGS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Create rankings dataframe\n",
        "rankings_data = []\n",
        "for supplier in ranked_suppliers:\n",
        "    criteria_scores = supplier.get('criteria_scores', {})\n",
        "    rankings_data.append({\n",
        "        'Rank': supplier.get('rank', 0),\n",
        "        'Supplier': supplier.get('supplier_name', 'Unknown'),\n",
        "        'Final Score': f\"{supplier.get('weighted_score', 0):.1f}\",\n",
        "        'Technical': criteria_scores.get('technical_compliance', {}).get('score', 0),\n",
        "        'Price': criteria_scores.get('price_competitiveness', {}).get('score', 0),\n",
        "        'Experience': criteria_scores.get('company_experience', {}).get('score', 0),\n",
        "        'Timeline': criteria_scores.get('timeline_feasibility', {}).get('score', 0),\n",
        "        'Risk': criteria_scores.get('risk_assessment', {}).get('score', 0)\n",
        "    })\n",
        "\n",
        "df_rankings = pd.DataFrame(rankings_data)\n",
        "display(df_rankings)\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Cell 13: Display Detailed Analysis\n",
        "\n",
        "View detailed analysis for each supplier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã DETAILED ANALYSIS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for supplier in ranked_suppliers:\n",
        "    print(f\"\\\\n{'#'*100}\")\n",
        "    print(f\"RANK #{supplier.get('rank', 0)}: {supplier.get('supplier_name', 'Unknown')}\")\n",
        "    print(f\"Final Score: {supplier.get('weighted_score', 0):.1f}/100\")\n",
        "    print(f\"{'#'*100}\\\\n\")\n",
        "    \n",
        "    # Overall summary\n",
        "    print(f\"üìù Overall Summary:\")\n",
        "    print(f\"   {supplier.get('overall_summary', 'N/A')}\\\\n\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"üí° Recommendations:\")\n",
        "    print(f\"   {supplier.get('recommendations', 'N/A')}\\\\n\")\n",
        "    \n",
        "    # Red flags\n",
        "    red_flags = supplier.get('red_flags', [])\n",
        "    if red_flags:\n",
        "        print(f\"‚ö†Ô∏è Red Flags:\")\n",
        "        for flag in red_flags:\n",
        "            print(f\"   ‚Ä¢ {flag}\")\n",
        "        print()\n",
        "    \n",
        "    # Criteria scores\n",
        "    print(f\"üìä Criteria Scores:\")\n",
        "    criteria_scores = supplier.get('criteria_scores', {})\n",
        "    for criterion, data in criteria_scores.items():\n",
        "        if isinstance(data, dict):\n",
        "            score = data.get('score', 0)\n",
        "            justification = data.get('justification', 'N/A')\n",
        "            criterion_name = criterion.replace('_', ' ').title()\n",
        "            print(f\"\\\\n   {criterion_name}: {score}/100\")\n",
        "            print(f\"   ‚Üí {justification}\")\n",
        "            \n",
        "            evidence = data.get('evidence', [])\n",
        "            if evidence:\n",
        "                print(f\"   Evidence:\")\n",
        "                for item in evidence[:3]:\n",
        "                    print(f\"      ‚Ä¢ {item}\")\n",
        "    \n",
        "    print(f\"\\\\n{'-'*100}\")\n",
        "\n",
        "print(\"\\\\n‚úÖ Detailed analysis displayed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Cell 14: Export Results to Excel\n",
        "\n",
        "Export the evaluation results to an Excel file and download it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì§ Exporting results to Excel...\\\\n\")\n",
        "\n",
        "# Generate timestamp for filename\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'tender_evaluation_results_{timestamp}.xlsx'\n",
        "\n",
        "# Export to Excel\n",
        "export_to_excel(ranked_suppliers, evaluation_weights, excel_filename)\n",
        "\n",
        "print(f\"‚úÖ Excel file created: {excel_filename}\")\n",
        "print(f\"\\\\nClick the button below to download:\\\\n\")\n",
        "\n",
        "# Download the file\n",
        "files.download(excel_filename)\n",
        "\n",
        "print(\"\\\\nüéâ Results exported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Cell 15: Save Results as JSON (Optional)\n",
        "\n",
        "Export results as JSON for programmatic access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üíæ Saving results as JSON...\\\\n\")\n",
        "\n",
        "# Create export data\n",
        "export_data = {\n",
        "    'evaluation_metadata': {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'weights_used': evaluation_weights,\n",
        "        'total_suppliers': len(ranked_suppliers),\n",
        "        'model_used': MODEL_NAME\n",
        "    },\n",
        "    'ranked_suppliers': ranked_suppliers\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "json_filename = f'tender_evaluation_results_{timestamp}.json'\n",
        "\n",
        "with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "    json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ JSON file created: {json_filename}\")\n",
        "print(f\"\\\\nClick the button below to download:\\\\n\")\n",
        "\n",
        "# Download the file\n",
        "files.download(json_filename)\n",
        "\n",
        "print(\"\\\\nüéâ JSON export completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Usage Instructions\n",
        "\n",
        "### Quick Start Guide:\n",
        "\n",
        "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
        "2. **Run All Cells**: Runtime ‚Üí Run all (or run cells sequentially)\n",
        "3. **Upload Files**: When prompted, select your tender and proposal documents\n",
        "4. **Review Results**: Scroll down to see rankings and detailed analysis\n",
        "5. **Download**: Click download buttons to get Excel and JSON files\n",
        "\n",
        "### Customization:\n",
        "\n",
        "- **Adjust Weights**: Modify Cell 9 to change evaluation criteria importance\n",
        "- **Change Model**: Edit Cell 3 to use a different model (e.g., Phi-3)\n",
        "- **Modify Criteria**: Update the analysis prompt in Cell 5\n",
        "\n",
        "### Troubleshooting:\n",
        "\n",
        "- **Out of Memory**: Use a smaller model or enable 4-bit quantization\n",
        "- **Slow Processing**: This is normal for local LLMs (2-5 min per proposal)\n",
        "- **Model Download Fails**: Try restarting the runtime\n",
        "- **GPU Not Available**: Check if you've enabled GPU in runtime settings\n",
        "\n",
        "---\n",
        "\n",
        "## üìû Support\n",
        "\n",
        "**JESA Tender Evaluation System**\n",
        "- GitHub: https://github.com/Kazaz-Mohammed/JESA_SUPPLIERS_MODEL.git\n",
        "- Version: Colab with Local LLM (Llama 3.2)\n",
        "\n",
        "---\n",
        "\n",
        "*Made with ‚ù§Ô∏è for JESA*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
